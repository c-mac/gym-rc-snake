{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x117c63370>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_batch_first = nn.LSTM(7, 20, batch_first=True)\n",
    "lstm = nn.LSTM(7, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-3.0575e-03,  2.4516e-02, -5.5056e-02,  5.2632e-02, -1.7257e-01,\n",
       "          -6.6691e-02, -3.8517e-03, -2.4449e-01, -7.1373e-02,  1.2206e-01,\n",
       "          -1.4211e-02,  1.4449e-01, -2.2072e-02, -6.8292e-02, -7.7594e-02,\n",
       "          -7.2419e-03, -4.6847e-02, -1.3369e-01, -9.8171e-02,  1.3531e-04]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq1 = [torch.randn(1, 7) for _ in range(5)]\n",
    "seq2 = [torch.randn(1, 7) for _ in range(5)]\n",
    "\n",
    "# So now let's say we want to batch our sequences!\n",
    "\n",
    "hidden = (torch.randn(1, 1, 20), torch.randn(1, 1, 20))\n",
    "\n",
    "for i in inputs:\n",
    "    out, hidden = lstm_batch_first(i.view(1, 1, -1), hidden)\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Alternatively, we can do the entire sequence all at once. the first value returned by LSTM is all of the hidden states throughout the sequence. the second is just the most recent hidden state compare the last slice of \"out\" with \"hidden\" below, they are the same). The reason for this is that:\n",
    "- \"out\" will give you access to all hidden states in the sequence\n",
    "- \"hidden\" will allow you to continue the sequence and backpropagate, by passing it as an argument  to the lstm at a later time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-8.5069e-02,  2.0289e-01,  1.8784e-02,  4.1801e-02,  2.7738e-01,\n",
       "            1.8410e-01,  2.1136e-01, -4.7201e-01,  2.2840e-01, -1.8833e-01,\n",
       "            1.1883e-01, -2.2269e-01, -3.2437e-01, -8.2023e-02, -3.0884e-01,\n",
       "           -9.7844e-02, -1.3080e-02,  1.0963e-02,  2.3678e-01,  1.0291e-01]],\n",
       " \n",
       "         [[ 7.3384e-02,  4.5436e-02,  3.1734e-02,  8.9404e-02,  2.5307e-01,\n",
       "            2.1927e-01, -4.7058e-03, -1.5095e-01,  8.3468e-02, -2.1347e-01,\n",
       "           -1.2955e-02, -1.2519e-01, -3.0005e-02, -5.7904e-02, -1.8288e-01,\n",
       "            5.0236e-02, -2.1838e-01,  1.1562e-02,  1.5974e-01,  6.8659e-03]],\n",
       " \n",
       "         [[ 9.6587e-02,  3.4265e-03,  2.6739e-02,  1.2856e-01,  2.0924e-01,\n",
       "            1.5437e-01,  7.6792e-03, -9.0873e-02,  5.6129e-02, -1.6116e-01,\n",
       "           -1.6234e-02, -1.5183e-01, -1.2519e-01, -5.4038e-02, -2.6785e-01,\n",
       "           -3.1699e-03, -1.0335e-01,  1.3382e-01,  2.5055e-01,  6.9618e-02]],\n",
       " \n",
       "         [[ 4.9389e-02,  2.6329e-02, -1.2304e-04,  9.7882e-02,  2.9207e-01,\n",
       "            1.6610e-01, -6.8071e-02, -1.5834e-01, -8.2348e-03, -1.6030e-01,\n",
       "            7.3718e-02, -5.2262e-02, -9.8867e-02, -4.7065e-02, -3.1801e-01,\n",
       "            2.5005e-02, -8.3417e-02,  1.9356e-01,  2.4844e-01,  1.5218e-01]],\n",
       " \n",
       "         [[ 1.6638e-01,  8.9526e-02,  1.3757e-01,  5.4232e-02,  3.2880e-01,\n",
       "            9.5059e-02, -1.5995e-01, -1.6464e-01, -1.2078e-01, -1.5739e-01,\n",
       "            1.0840e-01, -1.2286e-01, -7.2557e-02, -4.9307e-03, -3.7817e-01,\n",
       "            1.2105e-01, -8.2994e-02,  1.0782e-01,  2.3828e-01,  6.8179e-02]]],\n",
       "        grad_fn=<StackBackward>),\n",
       " (tensor([[[ 0.1664,  0.0895,  0.1376,  0.0542,  0.3288,  0.0951, -0.1599,\n",
       "            -0.1646, -0.1208, -0.1574,  0.1084, -0.1229, -0.0726, -0.0049,\n",
       "            -0.3782,  0.1210, -0.0830,  0.1078,  0.2383,  0.0682]]],\n",
       "         grad_fn=<StackBackward>),\n",
       "  tensor([[[ 0.2869,  0.2267,  0.2804,  0.0945,  0.5482,  0.1930, -0.3517,\n",
       "            -0.3545, -0.2010, -0.3107,  0.2182, -0.2205, -0.1335, -0.0137,\n",
       "            -0.7933,  0.1910, -0.1636,  0.1972,  0.4303,  0.1400]]],\n",
       "         grad_fn=<StackBackward>)))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "hidden = (torch.randn(1, 1, 20), torch.randn(1, 1, 20))  # clean out hidden state\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "\n",
    "(out, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
